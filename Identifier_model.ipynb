{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88694b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load a pre-trained model without the top layers\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add our own layers on top\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(train_generator.num_classes, activation='softmax') \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824a3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 354 images belonging to 10 classes.\n",
      "Found 82 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=20,       # Random rotation\n",
    "    width_shift_range=0.1,   # Random horizontal shift\n",
    "    height_shift_range=0.1,  # Random vertical shift\n",
    "    shear_range=0.2,         # Shear transformation\n",
    "    zoom_range=0.2,          # Zoom\n",
    "    horizontal_flip=True,    # Flip images horizontally\n",
    "    fill_mode='nearest'      # Filling strategy for new pixels\n",
    ")\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'D:\\Parts_reg\\heavy_machinery_parts_dataset',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'D:\\Parts_reg\\heavy_machinery_parts_dataset',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eab13f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Parts_reg\\venv\\lib\\site-packages\\PIL\\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - accuracy: 0.1840 - loss: 2.2491 - val_accuracy: 0.5488 - val_loss: 1.4674\n",
      "Epoch 2/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.5670 - loss: 1.3472 - val_accuracy: 0.7317 - val_loss: 0.9705\n",
      "Epoch 3/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.7152 - loss: 0.8770 - val_accuracy: 0.7805 - val_loss: 0.7837\n",
      "Epoch 4/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.8337 - loss: 0.5765 - val_accuracy: 0.7561 - val_loss: 0.6823\n",
      "Epoch 5/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.8668 - loss: 0.4551 - val_accuracy: 0.8171 - val_loss: 0.6354\n",
      "Epoch 6/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.9348 - loss: 0.3366 - val_accuracy: 0.7683 - val_loss: 0.5995\n",
      "Epoch 7/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.9393 - loss: 0.2584 - val_accuracy: 0.7927 - val_loss: 0.5903\n",
      "Epoch 8/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.9024 - loss: 0.3168 - val_accuracy: 0.8171 - val_loss: 0.5707\n",
      "Epoch 9/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.9642 - loss: 0.2355 - val_accuracy: 0.7683 - val_loss: 0.6046\n",
      "Epoch 10/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.9633 - loss: 0.1673 - val_accuracy: 0.7805 - val_loss: 0.5838\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94952aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('heavy_machine_parts_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff2e9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('heavy_machine_parts_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74dbf4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
      "Predicted Part: heavy_machinery_engine\n",
      "Part ID: 1007\n",
      "Part Name: Heavy Machinery Engine\n",
      "Description: Diesel engine for heavy-duty machines\n",
      "Manufacturer: Caterpillar\n",
      "Price: $15000\n",
      "Weight: 2200 kg\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load your parts database\n",
    "parts_df = pd.read_csv(\"parts_detail.csv\")  # make sure your CSV path is correct\n",
    "\n",
    "# Load label names\n",
    "labels = list(train_generator.class_indices.keys())\n",
    "\n",
    "# Mapping from label index to part_id\n",
    "label_to_part_id = {\n",
    "    0: 1001,  # axle_heavy_equipment\n",
    "    1: 1002,  # boom_arm_excavator\n",
    "    2: 1003,  # bulldozer_blade\n",
    "    3: 1004,  # crane_hook\n",
    "    4: 1005,  # excavator_bucket\n",
    "    5: 1006,  # gearbox_heavy_machinery\n",
    "    6: 1007,  # heavy_machinery_engine\n",
    "    7: 1008,  # hydraulic_cylinder\n",
    "    8: 1009,  # loader_tire\n",
    "    9: 1010,  # undercarriage_bulldozer\n",
    "}\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = image.load_img(\"D:/Parts_reg/test/hc/test6.jpg\", target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array /= 255.\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(img_array)\n",
    "predicted_class = np.argmax(predictions)\n",
    "predicted_label = labels[predicted_class]\n",
    "\n",
    "# Map to part_id\n",
    "predicted_part_id = label_to_part_id[predicted_class]\n",
    "\n",
    "# Fetch part details from the parts database\n",
    "part_details = parts_df[parts_df[\"part_id\"] == predicted_part_id]\n",
    "\n",
    "# Output\n",
    "if not part_details.empty:\n",
    "    part_info = part_details.iloc[0]\n",
    "    print(f\"Predicted Part: {predicted_label}\")\n",
    "    print(f\"Part ID: {part_info['part_id']}\")\n",
    "    print(f\"Part Name: {part_info['part_name']}\")\n",
    "    print(f\"Description: {part_info['description']}\")\n",
    "    print(f\"Manufacturer: {part_info['manufacturer']}\")\n",
    "    print(f\"Price: ${part_info['price (USD)']}\")\n",
    "    print(f\"Weight: {part_info['weight (kg)']} kg\")\n",
    "else:\n",
    "    print(\"Part details not found in database.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ce4d5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'axle_heavy_equipment': 0, 'boom_arm_excavator': 1, 'bulldozer_blade': 2, 'crane_hook': 3, 'excavator_bucket': 4, 'gearbox_heavy_machinery': 5, 'heavy_machinery_engine': 6, 'hydraulic_cylinder': 7, 'loader_tire': 8, 'undercarriage_bulldozer': 9}\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c235024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Predicted Part Details:\n",
      "Part Name: Engine\n",
      "Part Number: EN-003\n",
      "Price: 9500\n",
      "Supplier: Titan Motors\n",
      "Description: High torque diesel engine\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load parts database\n",
    "parts_df = pd.read_csv('parts_detail.csv')\n",
    "\n",
    "# Simulate model prediction\n",
    "predicted_class = 2  # (example: model output)\n",
    "predicted_part = parts_df.loc[parts_df['part_id'] == predicted_class]\n",
    "\n",
    "# Display the part details\n",
    "if not predicted_part.empty:\n",
    "    print(\"ğŸ” Predicted Part Details:\")\n",
    "    print(f\"Part Name: {predicted_part.iloc[0]['part_name']}\")\n",
    "    print(f\"Part Number: {predicted_part.iloc[0]['part_number']}\")\n",
    "    print(f\"Price: {predicted_part.iloc[0]['price']}\")\n",
    "    print(f\"Supplier: {predicted_part.iloc[0]['supplier']}\")\n",
    "    print(f\"Description: {predicted_part.iloc[0]['description']}\")\n",
    "else:\n",
    "    print(\"Part not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5924a0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
      "Collecting cachetools<6,>=4.0\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: packaging<25,>=20 in d:\\parts_reg\\venv\\lib\\site-packages (from streamlit) (24.2)\n",
      "Collecting watchdog<7,>=2.1.5\n",
      "  Downloading watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Collecting altair<6,>=4.0\n",
      "  Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in d:\\parts_reg\\venv\\lib\\site-packages (from streamlit) (11.1.0)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in d:\\parts_reg\\venv\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Collecting toml<2,>=0.10.1\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting click<9,>=7.0\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in d:\\parts_reg\\venv\\lib\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in d:\\parts_reg\\venv\\lib\\site-packages (from streamlit) (2.0.2)\n",
      "Collecting tenacity<10,>=8.1.0\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in d:\\parts_reg\\venv\\lib\\site-packages (from streamlit) (5.29.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in d:\\parts_reg\\venv\\lib\\site-packages (from streamlit) (4.13.1)\n",
      "Collecting blinker<2,>=1.0.0\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Collecting pyarrow>=7.0\n",
      "  Downloading pyarrow-19.0.1-cp39-cp39-win_amd64.whl (25.5 MB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in d:\\parts_reg\\venv\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Collecting narwhals>=1.14.2\n",
      "  Downloading narwhals-1.33.0-py3-none-any.whl (322 kB)\n",
      "Collecting jsonschema>=3.0\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Requirement already satisfied: colorama in d:\\parts_reg\\venv\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Collecting attrs>=22.2.0\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.24.0-cp39-cp39-win_amd64.whl (234 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\parts_reg\\venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\parts_reg\\venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\parts_reg\\venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\parts_reg\\venv\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\parts_reg\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\parts_reg\\venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\parts_reg\\venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\parts_reg\\venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\parts_reg\\venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Installing collected packages: rpds-py, attrs, referencing, smmap, jsonschema-specifications, narwhals, jsonschema, jinja2, gitdb, watchdog, toml, tenacity, pydeck, pyarrow, gitpython, click, cachetools, blinker, altair, streamlit\n",
      "Successfully installed altair-5.5.0 attrs-25.3.0 blinker-1.9.0 cachetools-5.5.2 click-8.1.8 gitdb-4.0.12 gitpython-3.1.44 jinja2-3.1.6 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 narwhals-1.33.0 pyarrow-19.0.1 pydeck-0.9.1 referencing-0.36.2 rpds-py-0.24.0 smmap-5.0.2 streamlit-1.44.1 tenacity-9.1.2 toml-0.10.2 watchdog-6.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'd:\\parts_reg\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
